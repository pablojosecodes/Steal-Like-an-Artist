{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d18c511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Total loss: 2028.26220703125\n",
      "Step 50, Total loss: 498.89141845703125\n",
      "Step 100, Total loss: 308.968017578125\n",
      "Step 150, Total loss: 236.876708984375\n",
      "Step 200, Total loss: 207.48672485351562\n",
      "Step 250, Total loss: 187.77772521972656\n",
      "Step 300, Total loss: 169.3457794189453\n",
      "Step 350, Total loss: 159.0277099609375\n",
      "Step 400, Total loss: 154.73887634277344\n",
      "Step 450, Total loss: 155.9910430908203\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from urllib.parse import urlparse\n",
    "#!/usr/bin/env python3\n",
    "# \"\"\"Neural style transfer in PyTorch.\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "# Define the VGGFeatures class\n",
    "class VGGFeatures(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = sorted(set(layers))\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                              std=[0.229, 0.224, 0.225])\n",
    "        self.model = models.vgg19(pretrained=True).features[:max(self.layers) + 1]\n",
    "        self.model.eval()\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.normalize(input)\n",
    "        features = {}\n",
    "        for i, layer in enumerate(self.model):\n",
    "            x = layer(x)\n",
    "            if i in self.layers:\n",
    "                features[i] = x\n",
    "        return features\n",
    "\n",
    "\n",
    "# Define the ContentLoss class\n",
    "class ContentLoss(nn.Module):\n",
    "    def __init__(self, target):\n",
    "        super().__init__()\n",
    "        self.target = target.detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.mse_loss(input, self.target)\n",
    "\n",
    "\n",
    "# Define the StyleLoss class\n",
    "class StyleLoss(nn.Module):\n",
    "    def __init__(self, target_feature):\n",
    "        super().__init__()\n",
    "        self.target = self.gram_matrix(target_feature).detach()\n",
    "\n",
    "    @staticmethod\n",
    "    def gram_matrix(input):\n",
    "        a, b, c, d = input.size()\n",
    "        features = input.view(a * b, c * d)\n",
    "        G = torch.mm(features, features.t())\n",
    "        return G.div(a * b * c * d)\n",
    "\n",
    "    def forward(self, input):\n",
    "        G = self.gram_matrix(input)\n",
    "        return F.mse_loss(G, self.target)\n",
    "\n",
    "\n",
    "# Define the function to load and preprocess the image\n",
    "\n",
    "def load_image(image_source, target_size):\n",
    "    # Check if the source is a URL or a local file\n",
    "    if urlparse(image_source).scheme in ('http', 'https'):\n",
    "        # Source is a URL\n",
    "        response = requests.get(image_source)\n",
    "        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    elif os.path.exists(image_source):\n",
    "        # Source is a local file path\n",
    "        image = Image.open(image_source).convert(\"RGB\")\n",
    "    else:\n",
    "        raise ValueError(\"Invalid image source. Must be a URL or a file path.\")\n",
    "\n",
    "    # Calculate aspect ratio resize\n",
    "    aspect_ratio = image.width / image.height\n",
    "    if aspect_ratio > 1:\n",
    "        # Image is wider than tall\n",
    "        new_height = target_size\n",
    "        new_width = int(target_size * aspect_ratio)\n",
    "    else:\n",
    "        # Image is taller than wide\n",
    "        new_width = target_size\n",
    "        new_height = int(target_size / aspect_ratio)\n",
    "\n",
    "    # Resize and center crop\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((new_height, new_width)),\n",
    "        transforms.CenterCrop(target_size),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    image = transform(image).unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "\n",
    "# Define content and style images URLs\n",
    "content_url = \"content.png\"\n",
    "style_url = \"grid.png\"\n",
    "\n",
    "# Load content and style images\n",
    "content_image = load_image(content_url, target_size=512)\n",
    "style_image = load_image(style_url, target_size=512)\n",
    "\n",
    "# Choose device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Transfer images to device\n",
    "content_image = content_image.to(device)\n",
    "style_image = style_image.to(device)\n",
    "\n",
    "# Define VGG layers\n",
    "content_layers = [22]\n",
    "style_layers = [1, 6, 11, 20, 29]\n",
    "all_layers = style_layers + content_layers\n",
    "vgg = VGGFeatures(all_layers).to(device)\n",
    "\n",
    "# Extract features\n",
    "content_features = vgg(content_image)\n",
    "style_features = vgg(style_image)\n",
    "\n",
    "# Initialize loss functions\n",
    "content_loss_fn = ContentLoss(content_features[content_layers[0]])\n",
    "style_loss_fns = [StyleLoss(style_features[layer]) for layer in style_layers]\n",
    "\n",
    "# Prepare output image and optimizer\n",
    "output_image = content_image.clone()\n",
    "optimizer = optim.Adam([output_image.requires_grad_()], lr=0.02)\n",
    "\n",
    "# Run the style transfer\n",
    "num_steps = 500\n",
    "style_weight = 1000000\n",
    "content_weight = 1\n",
    "\n",
    "for step in range(num_steps):\n",
    "    optimizer.zero_grad()\n",
    "    output_features = vgg(output_image)\n",
    "    content_loss = content_loss_fn(output_features[content_layers[0]])\n",
    "    style_loss = 0\n",
    "\n",
    "    for fn, layer in zip(style_loss_fns, style_layers):\n",
    "        style_loss += fn(output_features[layer])\n",
    "\n",
    "    total_loss = content_weight * content_loss + style_weight * style_loss\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "\n",
    "    if step % 50 == 0:\n",
    "        print(f\"Step {step}, Total loss: {total_loss.item()}\")\n",
    "        final_img = output_image.cpu().squeeze(0)\n",
    "        final_img = transforms.ToPILImage()(final_img.clamp(0, 1))\n",
    "        final_img.save(f\"output{step}.jpg\")\n",
    "\n",
    "# Save or display the final image\n",
    "final_img = output_image.cpu().squeeze(0)\n",
    "final_img = transforms.ToPILImage()(final_img.clamp(0, 1))\n",
    "final_img.save(\"output.jpg\")\n",
    "# final_img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32223f36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
