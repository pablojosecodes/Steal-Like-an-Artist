{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b76d6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "6\n",
      "11\n",
      "20\n",
      "29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                           | 1/500 [00:00<02:05,  3.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0554602997435723e-07\n",
      "0.0\n",
      "tensor(0., grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▏                                          | 2/500 [00:00<02:02,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.95248887280286e-07\n",
      "0.0018849928164854646\n",
      "tensor(0.0019, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▎                                          | 3/500 [00:00<02:01,  4.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5962227684649406e-07\n",
      "0.49015718698501587\n",
      "tensor(0.4902, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▎                                          | 4/500 [00:00<01:59,  4.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.607347144272353e-07\n",
      "0.25256726145744324\n",
      "tensor(0.2526, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▍                                          | 5/500 [00:01<01:59,  4.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4668920300664468e-07\n",
      "0.20890489220619202\n",
      "tensor(0.2089, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▌                                          | 6/500 [00:01<01:58,  4.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6538868408133567e-07\n",
      "0.15655061602592468\n",
      "tensor(0.1566, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▌                                          | 7/500 [00:01<01:58,  4.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7623489156903815e-07\n",
      "0.1275736540555954\n",
      "tensor(0.1276, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▋                                          | 8/500 [00:01<01:58,  4.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7726749490520888e-07\n",
      "0.10911277681589127\n",
      "tensor(0.1091, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▊                                          | 9/500 [00:02<01:58,  4.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7717744071887864e-07\n",
      "0.09765422344207764\n",
      "tensor(0.0977, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▊                                          | 9/500 [00:02<02:04,  3.95it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 205\u001b[0m\n\u001b[1;32m    203\u001b[0m         images\u001b[38;5;241m.\u001b[39mappend(output_image\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach())\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m#         display_image(output_image.clone().detach()\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mvgg\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m     loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.\u001b[39m)\n\u001b[1;32m    209\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[7], line 41\u001b[0m, in \u001b[0;36mVGG.forward\u001b[0;34m(self, input, layers)\u001b[0m\n\u001b[1;32m     37\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m layers:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# Efficient! Only get features from the layers we currently need\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m:\u001b[49m\u001b[43ml\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     index \u001b[38;5;241m=\u001b[39m l\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     43\u001b[0m     features[l]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    \n",
    "    # Note: layers = list of layers we want to get the features of\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Sort just in case\n",
    "        layers = sorted(set(layers))\n",
    "        \n",
    "        self.layers = layers\n",
    "        \n",
    "        # ImageNet normalization\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                              std=[0.229, 0.224, 0.225])\n",
    "\n",
    "        # Pretrained model- we only want the features and only those which include the layers we want \n",
    "        self.model = models.vgg19(pretrained=True).features[:layers[-1]+1]\n",
    "        self.model.eval()\n",
    "        self.model.requires_grad_(False)\n",
    "        \n",
    "        \n",
    "    def forward(self, input, layers=None):\n",
    "        # Sort or get default layer (for image)\n",
    "        layers = self.layers if layers is None else sorted(set(layers))\n",
    "        features = {}\n",
    "        \n",
    "        index = 0\n",
    "        \n",
    "        for l in layers:\n",
    "            # Efficient! Only get features from the layers we currently need\n",
    "            input = self.model[index:l+1](input)\n",
    "            index = l+1\n",
    "            features[l]=input\n",
    "        return features\n",
    "         \n",
    "        \n",
    "class ContentLoss(nn.Module):\n",
    "    def __init__(self, target):\n",
    "        super().__init__()\n",
    "        self.register_buffer('target', target)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Get the number of feature maps (channels) and the dimensions of each map\n",
    "        N = input.size(1)  # Number of feature maps\n",
    "        M = input.size(2) * input.size(3)  # Height times width of the feature map\n",
    "        \n",
    "        # Calculate the content loss\n",
    "        loss = F.mse_loss(input,self.target, reduction='sum')\n",
    "        # Normalize the loss by the number of elements in the feature maps\n",
    "        normalized_loss = loss / (2 * N * M)\n",
    "        return normalized_loss\n",
    "\n",
    "    \n",
    "    \n",
    "# 4d Tensor -> Gram Matrix\n",
    "# class GramMatrix(nn.Module):\n",
    "#     def forward(self, v):\n",
    "#         # Flatten\n",
    "#         v_f = v.flatten(-2)\n",
    "#         # Transpose (switch last two layers)\n",
    "#         v_f_t = v_f.transpose(-2, -1)\n",
    "#         # Matrix multiplication\n",
    "#         v_mul = v_f @ v_f_t\n",
    "#         # Normalize\n",
    "#         gram = v_mul / (v_mul.shape[0] * v_mul.shape[1])\n",
    "#         return gram\n",
    "class GramMatrix(nn.Module):\n",
    "    def forward(self, v):\n",
    "        # Get batch size, number of feature maps (channels), height, and width\n",
    "        b, c, h, w = v.size()\n",
    "        # Flatten the feature maps\n",
    "        v_f = v.view(b, c, h*w)\n",
    "        # Transpose the feature maps\n",
    "        v_f_t = v_f.transpose(1, 2)\n",
    "        # Compute the gram product\n",
    "        v_mul = torch.bmm(v_f, v_f_t)\n",
    "        # Normalize the gram matrix by dividing by the number of elements in each feature map\n",
    "        gram = v_mul / (c * h * w)\n",
    "        return gram\n",
    "\n",
    "\n",
    "# class StyleLoss(nn.Module):\n",
    "#     # Register target gram matrix for reuse\n",
    "#     def __init__(self, target_gram):\n",
    "#         super().__init__()\n",
    "#         self.register_buffer('target', target_gram)\n",
    "\n",
    "#     # Forward pass- Gram Matrix distance\n",
    "#     def forward(self, input):\n",
    "#         return nn.MSELoss()(GramMatrix()(input), self.target)\n",
    "class StyleLoss(nn.Module):\n",
    "    def __init__(self, target_gram):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = target_gram\n",
    "\n",
    "    def forward(self, G, input):\n",
    "\n",
    "        self.loss = nn.functional.mse_loss(G, self.target, reduction='sum')\n",
    "        N = input.size(0)\n",
    "        M = input.size(1) * input.size(2)  # Height times width of the feature map.\n",
    "        self.loss /= (4 * N * M)\n",
    "        return self.loss\n",
    "    \n",
    "    \n",
    "    \n",
    "class TVLoss(nn.Module):\n",
    "    def forward(self, input):\n",
    "        x_diff = input[..., :-1, :-1] - input[..., :-1, 1:]\n",
    "        y_diff = input[..., :-1, :-1] - input[..., 1:, :-1]\n",
    "        diff = x_diff**2 + y_diff**2\n",
    "        return torch.sum(diff / (input.shape[-2] * input.shape[-1]))\n",
    "\n",
    "    \n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "def display_image(tensor):\n",
    "    plt.imshow(tensor.squeeze().permute(1, 2, 0)  )\n",
    "\n",
    "def size_to_fit(size, max_dim, scale_up=False):\n",
    "    w, h = size\n",
    "    if not scale_up and max(h, w) <= max_dim:\n",
    "        return w, h\n",
    "    new_w, new_h = max_dim, max_dim\n",
    "    if h > w:\n",
    "        new_w = round(max_dim * w / h)\n",
    "    else:\n",
    "        new_h = round(max_dim * h / w)\n",
    "    return new_w, new_h\n",
    "\n",
    "\n",
    "# Function to load and preprocess the image\n",
    "def load_image(url, size=224):\n",
    "    response = requests.get(url)\n",
    "    image = Image.open(BytesIO(response.content))\n",
    "\n",
    "    # Transformation: Resize and center crop\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(size),  # Resize so the smaller side is size\n",
    "        transforms.CenterCrop(size),  # Center crop to the desired size\n",
    "        transforms.ToTensor(),  # Convert the PIL Image to a tensor\n",
    "    ])\n",
    "\n",
    "    image = transform(image).unsqueeze(0)  # Add a batch dimension\n",
    "    return image\n",
    "\n",
    "content_url = \"https://static1.smartbear.co/smartbearbrand/media/images/home/sb-hero-bg-img.jpg\"\n",
    "content_image = load_image(content_url)\n",
    "style_url = \"https://collectionapi.metmuseum.org/api/collection/v1/iiif/436535/796067/main-image\"\n",
    "style_image = load_image(style_url)\n",
    "\n",
    "\n",
    "content_layers = [22]\n",
    "content_weights = [350] \n",
    "\n",
    "style_layers = [1, 6, 11, 20, 29]\n",
    "style_weights = [256, 64, 16, 4, 1]\n",
    "layers = style_layers + content_layers\n",
    "\n",
    "vgg = VGG(layers=style_layers + content_layers)\n",
    "\n",
    "\n",
    "style_features = vgg(style_image, style_layers)\n",
    "style_targets = []\n",
    "for i in style_features.keys():\n",
    "    print(i)\n",
    "    style_targets.append(GramMatrix()(style_features[i]).detach())\n",
    "\n",
    "content_features = vgg(content_image, content_layers)\n",
    "content_targets = []\n",
    "for i in content_features.keys():\n",
    "    content_targets.append(content_features[i].detach())\n",
    "\n",
    "loss_fns = []\n",
    "for style_target in style_targets:\n",
    "    loss_fns.append(StyleLoss(style_target))\n",
    "for content_target in content_targets:\n",
    "    loss_fns.append(ContentLoss(content_target))\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "output_image = content_image.detach().clone()\n",
    "output_image.requires_grad_(True)\n",
    "style_weights = [1e3/n**2 for n in [64,128,256,512,512]]\n",
    "content_weights = [1e0]\n",
    "weights = style_weights  + content_weights\n",
    "from torch import optim\n",
    "losses = []\n",
    "images = []\n",
    "opt = optim.Adam([output_image], lr=5/255)\n",
    "for i in tqdm(range(500)):\n",
    "#     print(i)\n",
    "    if (i%100==0):\n",
    "        images.append(output_image.clone().detach())\n",
    "#         display_image(output_image.clone().detach()\n",
    "    features = vgg(output_image)\n",
    "\n",
    "    loss = torch.tensor(0.)\n",
    "\n",
    "    x = 0\n",
    "    style_loss_tot = 0\n",
    "    for layer in style_layers:\n",
    "        current_features = GramMatrix()(features[layer])\n",
    "        style_lossnow = weights[x] * loss_fns[x](current_features, features[layer])\n",
    "        style_loss_tot += style_lossnow\n",
    "        loss = loss +style_lossnow\n",
    "        x += 1\n",
    "        \n",
    "    print(style_loss_tot.item())\n",
    "#     x = len(style_layers)\n",
    "    content_loss_tot = 0\n",
    "    for layer in content_layers:\n",
    "        current_features = features[layer]\n",
    "        content_lossnow = weights[x] * loss_fns[x](current_features)\n",
    "        print(content_lossnow.item())\n",
    "        print(content_lossnow)\n",
    "        loss = loss +  content_lossnow \n",
    "        content_loss_tot = content_lossnow\n",
    "    \n",
    "#         x += 1\n",
    "#     print(content_loss_tot.item())\n",
    "        \n",
    "#     tv = TVLoss()(output_image)\n",
    "# #     print(tv.item())\n",
    "#     loss = loss + tv\n",
    "#     print(f\"x {loss.item()}\")\n",
    "    \n",
    "        \n",
    "    opt.zero_grad()\n",
    "#     losses.append(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    opt.step()\n",
    "#     print(f\"Loss {loss.item()}\")\n",
    "    with torch.no_grad():\n",
    "        output_image.clamp_(0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2ba8b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41edfa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e14f6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vz/lk5b1jfd4711vnp0_dg2fx080000gn/T/ipykernel_46241/1932211246.py:54: UserWarning: Using a target size (torch.Size([64, 64])) that is different to the input size (torch.Size([128, 128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return nn.functional.mse_loss(G, self.target)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (128) must match the size of tensor b (64) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 107\u001b[0m\n\u001b[1;32m    104\u001b[0m style_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fn, feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(style_loss_fns, output_features[\u001b[38;5;241m1\u001b[39m:]):\n\u001b[0;32m--> 107\u001b[0m     style_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m content_weight \u001b[38;5;241m*\u001b[39m content_loss \u001b[38;5;241m+\u001b[39m style_weight \u001b[38;5;241m*\u001b[39m style_loss\n\u001b[1;32m    111\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[8], line 54\u001b[0m, in \u001b[0;36mStyleLoss.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m     53\u001b[0m     G \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgram_matrix(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/functional.py:3294\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3292\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3294\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/functional.py:74\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (128) must match the size of tensor b (64) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the VGG class\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(VGG, self).__init__()\n",
    "        self.layers = sorted(set(layers))\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        self.model = models.vgg19(pretrained=True).features[:max(layers)+1]\n",
    "        self.model.eval()\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.normalize(x)\n",
    "        features = []\n",
    "        for i, layer in enumerate(self.model):\n",
    "            x = layer(x)\n",
    "            if i in self.layers:\n",
    "                features.append(x)\n",
    "        return features\n",
    "\n",
    "# Define the ContentLoss class\n",
    "class ContentLoss(nn.Module):\n",
    "    def __init__(self, target):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        self.target = target.detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return nn.functional.mse_loss(input, self.target)\n",
    "\n",
    "# Define the StyleLoss class\n",
    "class StyleLoss(nn.Module):\n",
    "    def __init__(self, target_feature):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = self.gram_matrix(target_feature).detach()\n",
    "\n",
    "    def gram_matrix(self, input):\n",
    "        a, b, c, d = input.size()  \n",
    "        features = input.view(a * b, c * d)  \n",
    "        G = torch.mm(features, features.t())  \n",
    "        return G.div(a * b * c * d)\n",
    "\n",
    "    def forward(self, input):\n",
    "        G = self.gram_matrix(input)\n",
    "        return nn.functional.mse_loss(G, self.target)\n",
    "\n",
    "# Define the function to load and preprocess the image\n",
    "def load_image(url, size=None, max_size=None):\n",
    "    response = requests.get(url)\n",
    "    image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "    if max_size is not None:\n",
    "        scale = max_size / max(image.size)\n",
    "        size = np.array(image.size) * scale\n",
    "        image = image.resize(size.astype(int), Image.ANTIALIAS)\n",
    "\n",
    "    if size is not None:\n",
    "        image = transforms.Resize(size)(image)\n",
    "\n",
    "    image = transforms.ToTensor()(image).unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "# Define content and style images\n",
    "content_url = \"https://static1.smartbear.co/smartbearbrand/media/images/home/sb-hero-bg-img.jpg\"\n",
    "content_image = load_image(content_url, size=(224, 224))\n",
    "style_url = \"https://collectionapi.metmuseum.org/api/collection/v1/iiif/436535/796067/main-image\"\n",
    "style_image = load_image(style_url, size=(224, 224))\n",
    "\n",
    "# Define VGG layers\n",
    "content_layers = [22]\n",
    "style_layers = [1, 6, 11, 20, 29]\n",
    "all_layers = style_layers + content_layers\n",
    "vgg = VGG(all_layers)\n",
    "\n",
    "# Get features\n",
    "content_features = vgg(content_image)\n",
    "style_features = vgg(style_image)\n",
    "\n",
    "# Initialize loss functions\n",
    "content_loss_fn = ContentLoss(content_features[0])\n",
    "style_loss_fns = [StyleLoss(style_feature) for style_feature in style_features]\n",
    "\n",
    "# Initialize output image and optimizer\n",
    "output_image = content_image.clone()\n",
    "optimizer = torch.optim.Adam([output_image.requires_grad_()], lr=0.01)\n",
    "\n",
    "# Run the style transfer\n",
    "num_steps = 300\n",
    "style_weight = 1e6\n",
    "content_weight = 1e0\n",
    "\n",
    "for step in range(num_steps):\n",
    "    output_features = vgg(output_image)\n",
    "    content_loss = content_loss_fn(output_features[0])\n",
    "    style_loss = 0\n",
    "\n",
    "    for fn, feature in zip(style_loss_fns, output_features[1:]):\n",
    "        style_loss += fn(feature)\n",
    "\n",
    "    total_loss = content_weight * content_loss + style_weight * style_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 50 == 0:\n",
    "        print(f\"Step {step}, Total loss: {total_loss.item()}\")\n",
    "\n",
    "# Display the final image\n",
    "plt.imshow(output_image.squeeze().permute(1, 2, 0).detach().numpy())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d081b21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
